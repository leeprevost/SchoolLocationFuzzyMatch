from sklearn.metrics import confusion_matrix, f1_score
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.feature_extraction.text import CountVectorizer
from classifier.classifier import pipeline
from confusion import plot_confusion_matrix

from termcolor import cprint
import argparse
import json
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


from utils import current_git_sha
from helpers import k12_clean
import os

DATA_DIR = 'Data'

TRAIN_FILE = 'ccd_sch_029_1617_w_0e_050317.csv'

def compare(eval_a_fn, eval_b_fn):
    """We're happy if eval_b is better than eval_a
    """

    eval_a, eval_b = None, None

    with open(eval_a_fn) as f:
        eval_a = json.load(f)

    with open(eval_b_fn) as f:
        eval_b = json.load(f)

    b_f1 = eval_b['weighted_f1_score']
    a_f1 = eval_a['weighted_f1_score']

    print("{: <24}  {: <22}   {: <22}   {: <22}".format('Comparing', eval_a_fn, eval_b_fn, 'diff'))
    print_comparison("Weighted F1 Scores", a_f1, b_f1)

    for metric in eval_a['class_f1_scores'].keys():
        print_comparison(metric,
                         eval_a['class_f1_scores'][metric],
                         eval_b['class_f1_scores'][metric])


def print_comparison(name, a, b):
    diff = b - a
    if diff > 0:
        color = 'green'
    elif diff == 0:
        color = 'white'
    else:
        color = 'red'

    cprint("{: <24}: {:.22f} {:.22f} {:.22f}".format(name, a, b, diff), color)


if __name__ == '__main__':
  
    
    
    parser = argparse.ArgumentParser(
        description='Evaluate the school classification model.')
    parser.add_argument('-t', '--train',
                        action='store',
                        default= os.path.join(DATA_DIR, TRAIN_FILE),
                        dest='train_file',
                        help='Dataset to train on')
    parser.add_argument('-e', '--test',
                        action='store',
                        dest='test_file',
                        help='Dataset to test on. If not set, a split of'
                             ' TRAIN_FILE will be used')
    parser.add_argument('-c', '--confusion-matrix',
                        action='store_const',
                        const=True,
                        default=False,
                        dest='confusion_matrix',
                        help='Compute a confusion matrix')
    parser.add_argument('-ch', "--chi2_select",
                        action="store_const", 
                        const=True,
                        default=False,
                        dest="select_chi2",
                        help="Select some number of features using a chi-squared test")
    
    args = parser.parse_args()

    np.random.seed(0)

    data = pd.read_csv(args.train_file, encoding='iso-8859-1', low_memory= False)
    
    
    
    
    x_cols = ['norm_SCH_NAME', 'ST']
    y_col = 'LEVEL'
    
    # add support and admin training data
    inc_data = pd.read_csv(os.path.join(DATA_DIR, 'support-admin training.csv'))
    
    #map and rename the columsn from inc_data to line up with data
    inc_data_cols = ['normLocDesc', 'class']
    data_cols = ['SCH_NAME', 'LEVEL']
    map = dict(zip(inc_data_cols, data_cols))
    inc_data.rename(columns = map, inplace=True)
    
    data = data.append(inc_data[data_cols], ignore_index=True)
    
    
    #add a normalized school name column.

    data['norm_SCH_NAME'] = k12_clean(data['SCH_NAME'])
    
    #convert ST columnt to category
    
    data['ST'] = data['ST'].astype("category")
    
    X = data[x_cols]
    y = data[y_col]
    unique_labels = y.unique()
    
    # save clean training file for future training    
    data.to_csv(os.path.join(DATA_DIR, "Train_clean_with_support.csv"), index = False)
    
    

    if args.test_file:
        pass
        train_X, train_y = X, y
        test_data = pd.read_csv(args.test_file, encoding='iso-8859-1')
        test_data['norm_SCH_NAME'] = k12_clean(test_data['SCH_NAME'])
        test_X = test_data[x_cols]
        test_y = test_data[y_col]
        
    else:
        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.20)
        # Take first split pair generated by splitter. If we were doing
        # multiple train/test splits, this would be invalid, but we'd need a
        # way to aggregate the scores for multiple splits.
        split = next(splitter.split(X, y))
        (train_idx, test_idx) = split
        train_X, train_y = X.iloc[train_idx], y[train_idx]
        test_X, test_y = X.iloc[test_idx], y[test_idx]
        
    if args.select_chi2:
        
        vectorizer = CountVectorizer(analyzer = "char_wb", ngram_range=(1,2))
        X_ch2 = vectorizer.fit_transform(train_X['norm_SCH_NAME'])
        
        feature_names = vectorizer.get_feature_names()
        
        ch2 = SelectKBest(chi2, k=1000)
        X_ch2 = ch2.fit_transform(X_ch2, train_y)
        ch2_df = pd.DataFrame(index = feature_names )
        ch2_df['scores']= ch2.scores_
        ch2_df['pvalues']= ch2.pvalues_
        ch2_df['support']= ch2.get_support()
        
        if feature_names:
            # keep selected feature names
            feature_names = [feature_names[i] for i
                             in ch2.get_support(indices=True)]
        

    pipeline.fit(train_X, train_y)
    y_true = test_y
    y_pred = pipeline.predict(test_X)

    weighted_f1_score = f1_score(y_true, y_pred, average='weighted')
    f1_scores = f1_score(y_true, y_pred,
                         average=None,
                         labels=unique_labels)
    class_f1_scores = dict(zip(unique_labels, f1_scores))

    evaluation = {
        'weighted_f1_score': weighted_f1_score,
        'class_f1_scores': class_f1_scores
    }

    filename = 'evaluation-{}.json'.format(current_git_sha())
    with open(filename, 'w') as f:
        json.dump(evaluation, f,
                  sort_keys=True,
                  indent=4,
                  separators=(',', ': '))

    compare('evaluation-best.json', filename)

    if args.confusion_matrix:
        cm = confusion_matrix(y_true, y_pred, labels=unique_labels)
        plot_confusion_matrix(cm, unique_labels, normalize=True)
        plt.show()
